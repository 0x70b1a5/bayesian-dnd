---
title: "A Bayesian Analysis of My D&D Dice"
author: "Everett Robinson"
date: "September 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

# Introduction

My girlfriend and I have been attending a weekly Dungeons & Dragons night at a local board game cafe, which means that we have been thoroughly immersed in the superstition surrounding the dice. We have heard stories of unlikely strings of bad rolls that ultimately lead to the frustrated fellow adventurer performing the [salt water test](https://www.youtube.com/watch?v=VI3N4Qg-JZM) to see if the dice are balanced poorly. This test is really cool, and it certainly explains the physical mechanism for why a die will come up on some faces more than others, but it doesn't tell you anything about the probabilities of rolling each face. Even with an off balance die, say a d20 that's weighted in favour of rolling a twenty, we shouldn't expect every normal roll on a hard surface to come up twenty. How frequently it comes up will presumably depend on the severity of the balance issue. As an aspiring data scientist I couldn't help but think that a much better approach would be a Bayesian analysis for estimating the probability of rolling each face. 

Normally I don't care too much about such superstitions and just take the rolls as they come. I probably never would have implemented this Bayesian analysis, if not for the fact that I have developed a bit of a reputation for rolling high damage numbers on my d10 at our table. The curiosity has now gotten the better of me and I want better estimates of the true probabilities for my dice.


# Methodology

There are a couple different ways I can think of to approach the problem of assessing if my dice are correctly balanced. I could always use a simple frequentist hypothesis test and set a null hypothesis of **p** being equal to the fair probability for each face. I could then roll the dice many times and see if the observed data provides enough evidence to reject that hypothesis for any of the faces. This is a nice and simple approach, which would be straight forward to test using the binomial distribution. I would run one hypothesis test for each face, and adjust the data for each test to be a success if the face comes up, and a failure if it is any other face. All that remains to be done after that is to check the probability of seeing our observed number of successes or a more extreme result, and deciding if that falls below our chosen significance level. There are a couple of drawbacks to this approach that I can immediately think of:

<ol> 
  <li>It tells us nothing about what a better estimate of the probability for each face is; and</li>
  <li>We will likely need a lot more data to achieve significance using this frequentist approach.</li>
</ol>

Instead, I think a Bayesian approach will be much more informative. We will still each treat roll as a Bernoulli random variable for each face (either a one for that face showing up, or a zero for any other face), and we will treat the sequence of these as a binomial distribution. This time, instead of picking a single value of **p** for the probability of a face, we will define a prior distribution across all the possibilities of **p** from 0 to 1. This prior distribution represents the probability density function (pdf) of our belief in the probabilities for each of the values of **p** across the range. We then compute the probability density for each value of **p** in our prior *given* the binomial distribution we have observed as evidence in favour for or against it. This resulting pdf for **p** is called the posterior distribution, and it represents our updated belief in the probability that any of the possible values of **p** is true. The question now is how will we define this prior distribution for **p**?

We are in luck, because there is a very versatile prior distribution available to us that has a simple analytical solution for calculating posterior probabilities when working with a binomial likelihood: The beta function! The beta function is something called a conjugate prior for the binomial distribution, which is really just a fancy way of saying that if we pick a beta prior, the posterior for our values of **p** is simply another beta distribution with simple rules for updating it based and the observed number of successes and failures. In this case it will be:

$Beta(\alpha 0 + successes,\beta 0 + failures)$

Where $\alpha 0$ and $\beta 0$ are parameters of our prior beta distribution. Before we get started we will need to determine a reasonable beta prior to use. For the sake of simplifying my explanation, let's consider the analysis of a twenty sided dice for the rest of this section. If we honestly knew nothing about dice, we might decide that any value of **p** is just as likely as the rest, and assign a uniform prior distribution. This can be achieved with $Beta(1,1)$:

```{r, Beta(1,1)}
ggplot(data.frame(x = c(0:1))) + 
  stat_function(fun = dbeta, 
                args = list(shape1 = 1, shape2 = 1)) +
  stat_function(fun = dbeta, 
                args = list(shape1 = 1, shape2 = 1),
                xlim = c(0,1),
                geom = "area",
                aes(x, alpha = 0.5)) +
  labs(title = "Beta(1,1) Distribution",
       x = "Binomial probability, p",
       y = "Probability density of p") +
  theme(legend.position="none")
```

We will go forward assuming that we know a little bit about dice, and that our prior belief in the likelihood of any given face is based on the dice being fair. For our d20, this means we expect **p = 0.05**. To model this as a prior we could simply say that we expect 1 roll out of twenty to be our face, and model it with $Beta(1,19)$:

```{r, Beta(1,19)}
ggplot(data.frame(x = c(0:1))) + 
  stat_function(fun = dbeta, 
                args = list(shape1 = 1, shape2 = 19)) +
  stat_function(fun = dbeta, 
                args = list(shape1 = 1, shape2 = 19),
                xlim = c(0,1),
                geom = "area",
                aes(x, alpha = 0.5)) +
  labs(title = "Beta(1,19) Distribution",
       x = "Binomial probability, p",
       y = "Probability density of p") +
  theme(legend.position="none")
```

Notice that this isn't a very confident prior distribution. We can see a decently likelihood for values of **p** anywhere between 0 and 0.25. If we want a stronger prior we can use larger values of $\alpha$ and $\beta$ where $\alpha / (\alpha + \beta) = 0.05$. Let's look at $Beta(5,95)$:

```{r, Beta(5,95)}
ggplot(data.frame(x = c(0:1))) + 
  stat_function(fun = dbeta, 
                args = list(shape1 = 5, shape2 = 95)) +
  stat_function(fun = dbeta, 
                args = list(shape1 = 5, shape2 = 95),
                xlim = c(0,1),
                geom = "area",
                aes(x, alpha = 0.5)) +
  labs(title = "Beta(5,95) Distribution",
       x = "Binomial probability, p",
       y = "Probability density of p") +
  theme(legend.position="none")
```

That's a bit better. There is a definite peak around **p = 0.05** with a much tighter spread. The power of Bayesian statistics is that we can continually work with uncertainty in our estimates, and that we treat **p** as a probability density function rather than a point estimate. If we wanted to find some common summary statistics for **p** however, we could do it with the following formulas and functions:

$Mean = \frac{\alpha}{\alpha + \beta}$

$Median = qbeta(0.5, \alpha, \beta)$

$Mode = \frac{\alpha - 1}{\alpha + \beta -2}$

More important than these summary statistics, however, is our ability to calculate a credible interval for the beta distribution. This gives us something similar to a frequentist confidence interval, except that we can say that we believe there is a certain probability that the interval contains the true parameter (in this case, p) given our assumptions (our prior). Much like the median, the credible interval can be found by passing the appropriate quantiles to the qbeta function for our posterior. For a 95% credible interval, these would be 0.025 for the lower bound an 0.975 for the upper bound.

I think that's enough background for now. Let's start analyzing!


# Analysis

Rather than duplicating a bunch of code for each dice, I will just start by writing a function that can be reused:

```{r}

```

